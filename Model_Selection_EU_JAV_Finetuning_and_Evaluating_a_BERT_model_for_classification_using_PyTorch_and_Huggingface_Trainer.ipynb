{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model Selection EU-JAV Finetuning and Evaluating a BERT model for classification - using PyTorch and Huggingface Trainer ",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cheatham1/EU-JAV/blob/main/Model_Selection_EU_JAV_Finetuning_and_Evaluating_a_BERT_model_for_classification_using_PyTorch_and_Huggingface_Trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FfqM7xWqzBK"
      },
      "source": [
        "## EU-JAV - Finetuning and Evaluating a BERT model for Classification\n",
        "\n",
        "\n",
        "\n",
        "In this notebook we will finetune BERT base models. \n",
        "\n",
        "\n",
        "After training, we will save the model, evaluate it and use it for predictions.\n",
        "\n",
        "\n",
        "Thanks to Per from the National Library of Norway"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Runtime > Change runtime type menu - GPU\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "JPpwbycUZVt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JCcC7lE8kCy"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz_FIbsIqfGu"
      },
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install sentencepiece\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "from transformers import EarlyStoppingCallback\n",
        "#from transformers import BertForPreTraining\n",
        "#from transformers import RobertaTokenizer, RobertaModel, RobertaForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM\n",
        "#from transformers import CamembertModel, CamembertTokenizer\n",
        "#from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "from transformers import RobertaTokenizer, XLMRobertaXLForSequenceClassification\n",
        "\n",
        "#from transformers import ElectraForSequenceClassification\n",
        "#from transformers import XLMTokenizer, XLMWithLMHeadModel\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LwbdSXsVCNO"
      },
      "source": [
        "#@markdown Set the main model that the training should start from\n",
        "#\n",
        "#model_name = 'bert-base-multilingual-uncased' #@param [\"NbAiLab/nb-bert-base\", \"NbAiLab/nb-bert-large\", \"bert-base-multilingual-cased\", \"bert-base-multilingual-uncased\"]\n",
        "#model_name = 'dbmdz/bert-base-italian-xxl-uncased' #@param [\"dbmdz/bert-base-italian-xxl-uncased\", \"dbmdz/bert-base-italian-xxl-cased\",\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\",\"bert-base-multilingual-cased\",\"bert-base-multilingual-uncased\"]\n",
        "#model_name = 'digitalepidemiologylab/covid-twitter-bert-v2'\n",
        "#model_name = 'dbmdz/bert-base-italian-xxl-uncased' #@param [\"dbmdz/bert-base-italian-xxl-uncased\",\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\",\"bert-base-uncased\", \"bert-base-multilingual-cased\"]\n",
        "#model_name = 'xlm-roberta-base' #@param['roberta-base','xlm-roberta-base','roberta-large-mnli','cardiffnlp/twitter-roberta-base', 'dbmdz/bert-base-italian-uncased']\n",
        "\n",
        "\n",
        "#model_name = \"google/electra-small-discriminator\"\n",
        "#model_name = \"dbmdz/electra-base-italian-xxl-cased-discriminator\"\n",
        "model_name = \"xlm-roberta-large\"\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown Set training parameters\n",
        "batch_size =  16 #16@param {type: \"integer\"} \n",
        "learning_rate = 2e-5 #@param {type: \"number\"}\n",
        "warmup_proportion = 0.15 #@param {type: \"number\"}\n",
        "\n",
        "num_epochs = 13 #@param {type: \"integer\"} # 13\n",
        "max_seq_length = 96 #256 , 128, 98, 128@param {type: \"integer\"}\n",
        "weight_decay = 0.01 #@param {type: \"number\"} # 0.01\n",
        "\n",
        "\n",
        "#tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "#model = BertModel.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# xlm-roberta-large and base **\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
        "model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnse7vWrGNVe"
      },
      "source": [
        "addData = False  # False = use just dataset 1 True = use dataset1+2\n",
        "#useRemovedDifficultTrainingData = False # datset saved after initial run to remove difficult tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = tokenizer.convert_ids_to_tokens(range(tokenizer.vocab_size))\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "id": "nTl3XOvXUILJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#words = [\"maschera\", \"mask\", \"covid\", \"coronavirus\", \"virus\", \"isolation\", \"confinement\", \"vaccination\", \"vaccine\"]\n",
        "#for word in words:\n",
        "#  print(word, (word in vocab))"
      ],
      "metadata": {
        "id": "Qfr5Y4-nXDDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(len(tokenizer.vocab))"
      ],
      "metadata": {
        "id": "bl0Q6Mc0Q25w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkHx8JHYWTU6"
      },
      "source": [
        "## Load and Prepare the Dataset used for Finetuning\n",
        "The selected dataset is loaded directly from a web resource. It is coded with labels and text in a comma-separated file. You can replace this with any other data source. This data is here converted into the pytorch data format. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rdOwBNvBVGa"
      },
      "source": [
        "# ----- Data set 1 Aleady split into train, dev, test-----#\n",
        "\n",
        "train_data1 = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/cheatham1/EU-JAV-AB/main/3categories/datasetA_train_3categories.csv',\n",
        "    names=['Annotator1','Annotator2','Annotator3','label','text','index']\n",
        ")\n",
        "dev_data1 = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/cheatham1/EU-JAV-AB/main/3categories/datasetA_dev_3categories.csv',\n",
        "    names=['Annotator1','Annotator2','Annotator3','label','text','index']\n",
        ")\n",
        "test_data1 = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/cheatham1/EU-JAV-AB/main/3categories/datasetA_test_3categories.csv',\n",
        "    names=['Annotator1','Annotator2','Annotator3','label','text','index']\n",
        ")\n",
        "\n",
        "print(\"Dataset1: \", train_data1.shape, dev_data1.shape, test_data1.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Data set 2 Aleady split into train, dev, test-----#\n",
        "\n",
        "train_data2 = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/cheatham1/EU-JAV-AB/main/3categories/datasetB_train_3categories.csv',\n",
        "    names=['Annotator1','Annotator2','Annotator3','label','text','index']\n",
        "    )\n",
        "\n",
        "dev_data2 = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/cheatham1/EU-JAV-AB/main/3categories/datasetB_dev_3categories.csv',\n",
        "    names=['Annotator1','Annotator2','Annotator3','label','text','index']\n",
        "    )\n",
        "\n",
        "test_data2 = pd.read_csv( \n",
        "    'https://raw.githubusercontent.com/cheatham1/EU-JAV-AB/main/3categories/datasetB_dev_3categories.csv',\n",
        "    names=['Annotator1','Annotator2','Annotator3','label','text','index']\n",
        "    )"
      ],
      "metadata": {
        "id": "Sm7E8S1vw-FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN6pZAPiiq24"
      },
      "source": [
        "if addData == True:\n",
        "  print(\"adding datasets\")\n",
        "   \n",
        "  train_data = train_data1.append(train_data2)\n",
        "  dev_data  = dev_data1.append(dev_data2)\n",
        "  test_data  = test_data1.append(test_data2)\n",
        "\n",
        "else:\n",
        "  train_data= train_data1.copy()\n",
        "  dev_data= dev_data1.copy()\n",
        "  test_data= test_data1.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.groupby(['label']).count()"
      ],
      "metadata": {
        "id": "8GFdOVC2wRh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install emoji\n",
        "#import emoji\n",
        "#train_data['text'] = train_data['text'].apply(lambda x: emoji.demojize(x))\n",
        "#dev_data['text']   = dev_data['text'].apply(lambda x: emoji.demojize(x))\n",
        "#test_data['text']  = test_data['text'].apply(lambda x: emoji.demojize(x))"
      ],
      "metadata": {
        "id": "su5XCoO6ztXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K73F7jdKJAwP"
      },
      "source": [
        "train_data.groupby(['label']).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6SbWl-4J9i0"
      },
      "source": [
        "dev_data.groupby(['label']).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOIPkZkmnK8l"
      },
      "source": [
        "print(f'The dataset is imported.\\n\\nThe training dataset has {len(train_data)} items.\\nThe development dataset has {len(dev_data)} items. \\nThe test dataset has {len(test_data)} items')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H92Asgy_Hdz"
      },
      "source": [
        "train_data.label = train_data.label.astype('float').astype('Int64')\n",
        "dev_data.label = dev_data.label.astype('float').astype('Int64')\n",
        "test_data.label = test_data.label.astype('float').astype('Int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvwMibpe1NEC"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data['text'] = train_data.text.str.replace('\\n',' ')\n",
        "#dev_data['text'] = dev_data.text.str.replace('\\n',' ')\n",
        "#test_data['text'] = test_data.text.str.replace('\\n',' ')"
      ],
      "metadata": {
        "id": "d23fXvyuMMbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te38-MEeA-2_"
      },
      "source": [
        "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
        "dev_data = dev_data.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3Ik6H_eMgbi"
      },
      "source": [
        "# ----- Preprocess data -----#\n",
        "# Preprocess data\n",
        "X_train = list(train_data[\"text\"])\n",
        "y_train = list(train_data[\"label\"])\n",
        "X_dev = list(dev_data[\"text\"])\n",
        "y_dev = list(dev_data[\"label\"])\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
        "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=max_seq_length)\n",
        "X_dev_tokenized = tokenizer(X_dev, padding=True, truncation=True, max_length=max_seq_length)\n",
        "\n",
        "# Create torch dataset\n",
        "\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels:\n",
        "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "train_dataset = Dataset(X_train_tokenized, y_train)\n",
        "val_dataset = Dataset(X_dev_tokenized, y_dev)\n",
        "\n",
        "print(f'The dataset is imported.\\n\\nThe training dataset has {len(train_data)} items.\\nThe development dataset has {len(dev_data)} items. \\nThe test dataset has {len(test_data)} items')\n",
        "steps = round(len(train_data)/batch_size)\n",
        "\n",
        "num_warmup_steps = round(steps*warmup_proportion*num_epochs)\n",
        "print(f'You are planning to train for a total of {steps} steps * {num_epochs} epochs = {num_epochs*steps} steps. Warmup is {num_warmup_steps} steps or {round(100*num_warmup_steps/(steps*num_epochs))}%. We recommend at least 10%.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7WNLD_jP3OI"
      },
      "source": [
        "totalsize = len(train_data) + len(dev_data) + len(test_data)\n",
        "print(f'The dataset total size {totalsize}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train[0]"
      ],
      "metadata": {
        "id": "jVC0QfN9JEQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYVyn0Q3W1sg"
      },
      "source": [
        "# Start Training\n",
        "We are here using the HuggingFace Trainer interface. An alternative implementation could be to use Tensorflow/Keras or native PyTorch.\n",
        "\n",
        "Please note that training the large BERT-model on a GPU might be a challenge. The two critical parameters are batch_size and sequence_length. Reduce these until you no longer are getting Out-of-memory(OOM) errors. The political speeches corpus above have very long sequences. You might want to truncate them at 128 tokens. This makes the task harder since the model is allowed to see less of each sequence. Reducing batch_size below 8 might lead to unstability and very long training time."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from transformers.optimization import Adafactor, AdafactorSchedule\n",
        "\n",
        "#optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
        "#lr_scheduler = AdafactorSchedule(optimizer)"
      ],
      "metadata": {
        "id": "oBHEtQ_npkHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n",
        "#            position_ids=None, head_mask=None, labels=None):"
      ],
      "metadata": {
        "id": "ltz_FkeRpwzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKM8Elz-Uw95"
      },
      "source": [
        "# ----- Fine-tune pretrained model -----#\n",
        "# Define Trainer parameters\n",
        "def compute_metrics(p):\n",
        "    pred, label = p\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true=label, y_pred=pred)\n",
        "\n",
        "    recall = recall_score(y_true=label, y_pred=pred, average='weighted')\n",
        "    precision = precision_score(y_true=label, y_pred=pred, average='weighted')\n",
        "    f1 = f1_score(y_true=label, y_pred=pred, average='weighted')\n",
        "\n",
        "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "# Define Trainer\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"output\",\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    #evaluation_strategy=\"steps\",\n",
        "    #eval_steps=round(steps/2),\n",
        "    #logging_steps=round(steps/10),\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    learning_rate=learning_rate, #The default here is linear decay to 0. \n",
        "    warmup_steps=num_warmup_steps,\n",
        "    num_train_epochs=num_epochs,\n",
        "    weight_decay = weight_decay,\n",
        "    #save_steps=steps, #Only saves at the end\n",
        "    #seed= 3, #3,\n",
        "    metric_for_best_model= \"accuracy\",\n",
        "    load_best_model_at_end=True,\n",
        "    #push_to_hub=True,\n",
        "    #push_to_hub_model_id=f\"{model_name}-finetuned-EUJAV\",\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    #optimizers=(optimizer, lr_scheduler),  ##### do we want this?\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "    #callbacks = [tboard_callback]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Train pre-trained model\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zdyVwO0-Xtjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "#files.download('output/checkpoint-66/trainer_state.json')"
      ],
      "metadata": {
        "id": "GEKUwSyK29Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(model)"
      ],
      "metadata": {
        "id": "aYiVRdsw_Moe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_m5laKFlsNs"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE4sY-cpXUEF"
      },
      "source": [
        "# Run Preditions and print Evaluation Report\n",
        "The code below first runs predictions on the train dataset. After that it prints and evaluation report using a tool from sklearn.\n",
        "\n",
        "Typically it is two number you want from this: The accuracy score (the first number on the \"accuracy\"-line. In addtion most journals want you to report the F1-macro-score since this is a sequence classification task. This is the number beneath accuracy (or in the intersection between f1-score and macro-avg).\n",
        "\n",
        "One of the tasks above is a balanced dataset. Both are binary classification. In a balanced binary classification the F1-macro and the average is basically the same (rounding differences only). In the unbalanced set, these values will vary greatly. The F1-macro is typically a much better measurement of how good your network is doing.\n",
        "\n",
        "We repeat the same raport for eval and test as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItbQEO1OUdHj"
      },
      "source": [
        "#Print report\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"\\nValidation-set Evaluation\")\n",
        "dev_dataset = Dataset(X_dev_tokenized)\n",
        "dev_pred, _, _ = trainer.predict(dev_dataset)\n",
        "y_pred_bool_dev = np.argmax(dev_pred, axis=1)\n",
        "#print(classification_report(dev_data[\"label\"], y_pred_bool, digits=3))\n",
        "print(classification_report(dev_data[\"label\"].to_numpy().astype(\"int\"), y_pred_bool_dev, digits=3))\n",
        "\n",
        "\n",
        "print(\"\\nTrain-set Evaluation\")\n",
        "train_dataset = Dataset(X_train_tokenized)\n",
        "train_pred, _, _ = trainer.predict(train_dataset)\n",
        "y_pred_bool_train = np.argmax(train_pred, axis=1)\n",
        "#print(classification_report(train_data[\"label\"], y_pred_bool, digits=3))\n",
        "print(classification_report(train_data[\"label\"].to_numpy().astype(\"int\"), y_pred_bool_train, digits=3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_label_prediction = train_data.copy()\n",
        "df_label_prediction['prediction'] = y_pred_bool_train.tolist()"
      ],
      "metadata": {
        "id": "5BVK30f_xCal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_label_prediction.head()"
      ],
      "metadata": {
        "id": "jc2blTBAvUxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number wrongly labelled \n",
        "indexNames = df_label_prediction[ df_label_prediction['label'] != df_label_prediction['prediction'] ].index\n",
        "print(len(indexNames))"
      ],
      "metadata": {
        "id": "sFEg4JUorHH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in indexNames:\n",
        "    print(i, df_label_prediction.text.iloc[i], \" : \",df_label_prediction.label.iloc[i], \" : \",df_label_prediction.prediction.iloc[i],)"
      ],
      "metadata": {
        "id": "EO7mSkYWvk_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "id": "ztJWMzuA1QzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "df_label_prediction.to_csv('trainingDataLabelsPredictions.csv') \n",
        "#files.download('trainingDataLabelsPredictions.csv')"
      ],
      "metadata": {
        "id": "2zvEApHXyB2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RqxJlL69ZUiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove difficult to label data from training dataset\n",
        "#indexNames = df_label_prediction[ df_label_prediction['label'] != df_label_prediction['prediction'] ].index\n",
        "\n",
        "# Remove easy to label data from training dataset\n",
        "#indexNames = df_label_prediction[ df_label_prediction['label'] == df_label_prediction['prediction'] ].index\n",
        "\n",
        "#df_label_prediction.drop(indexNames , inplace=True)\n",
        "#size=df_label_prediction.shape[0]\n",
        "#orig_size = train_data.shape[0]\n",
        "#print(size, (orig_size-size)/orig_size)"
      ],
      "metadata": {
        "id": "OrKxHpoG2mwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkTxnFiE6qvq"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1  = f1_score(y_dev, y_pred_bool_dev, average='macro')\n",
        "f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "acc = accuracy_score(y_dev, y_pred_bool_dev)\n",
        "acc"
      ],
      "metadata": {
        "id": "eldhzKrBeovf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateModelwDataset(dataset):\n",
        "\n",
        "  X_dev = list(dataset[\"text\"])\n",
        "  X_dev_tokenized = tokenizer(X_dev, padding=True, truncation=True, max_length=max_seq_length)\n",
        "\n",
        "  dev_dataset = Dataset(X_dev_tokenized)\n",
        "  dev_pred, _, _ = trainer.predict(dev_dataset)\n",
        "  y_pred_dev = np.argmax(dev_pred, axis=1)\n",
        "  y_dev = dataset[\"label\"].to_numpy().astype(\"int\")\n",
        "\n",
        "\n",
        "  print(classification_report(y_dev, y_pred_dev, digits=3))\n",
        "\n",
        "  f1_dataset = f1_score(y_dev, y_pred_dev, average='macro')\n",
        "  acc_dataset = accuracy_score(y_dev, y_pred_dev)\n",
        "\n",
        "  print(\"f1: \",f1_dataset, \"  accuracy: \",acc_dataset)\n",
        "\n",
        "  results = [f1_dataset, acc_dataset]\n",
        "\n",
        "  return(results)\n"
      ],
      "metadata": {
        "id": "KwD33IlUiXG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6KWNa70DoWrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval with dataset A. \n",
        "results_dataset1 = evaluateModelwDataset(dev_data1)"
      ],
      "metadata": {
        "id": "CdxOLJS6i8wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval with dataset B. \n",
        "results_dataset2 = evaluateModelwDataset(dev_data2)\n"
      ],
      "metadata": {
        "id": "DwIWQGDsi8-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test with dataset A+B \n",
        "dev_data3 = dev_data1.copy()\n",
        "dev_data3 = dev_data3.append(dev_data2)\n",
        "dev_data3 = dev_data3.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "results_dataset3 =evaluateModelwDataset(dev_data3)\n"
      ],
      "metadata": {
        "id": "ZEKDfJeAjKEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE2cmrPz61_W"
      },
      "source": [
        "print(\"Results from validation dataset\")\n",
        "\n",
        "print(\"F1\")\n",
        "\n",
        "print(\"DatasetA: {:.3f}\".format( results_dataset1[0]))\n",
        "print(\"DatasetB: {:.3f}\".format( results_dataset2[0]))\n",
        "print(\"DatasetA+B: {:.3f}\".format(results_dataset3[0]))\n",
        "\n",
        "print(\"Accuracy\")\n",
        "\n",
        "print(\"DatasetA: {:.3f}\".format( results_dataset1[1]))\n",
        "print(\"DatasetB: {:.3f}\".format( results_dataset2[1]))\n",
        "print(\"DatasetA+B: {:.3f}\".format(results_dataset3[1]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval with test data\n",
        "results_test_dataset1 = evaluateModelwDataset(test_data1)\n",
        "results_test_dataset2 = evaluateModelwDataset(test_data2)\n",
        "\n",
        "test_data3 = test_data1.copy()\n",
        "test_data3 = test_data3.append(test_data2)\n",
        "test_data3 = test_data3.sample(frac=1).reset_index(drop=True)\n",
        "results_test_dataset3 = evaluateModelwDataset(test_data3)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JqEd_VYT-yXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Results from TEST dataset\")\n",
        "\n",
        "print(\"F1\")\n",
        "\n",
        "print(\"DatasetA: {:.3f}\".format( results_test_dataset1[0]))\n",
        "print(\"DatasetB: {:.3f}\".format( results_test_dataset2[0]))\n",
        "print(\"DatasetA+B: {:.3f}\".format(results_test_dataset3[0]))\n",
        "\n",
        "print(\"Accuracy\")\n",
        "\n",
        "print(\"DatasetA: {:.3f}\".format( results_test_dataset1[1]))\n",
        "print(\"DatasetB: {:.3f}\".format( results_test_dataset2[1]))\n",
        "print(\"DatasetA+B: {:.3f}\".format(results_test_dataset3[1]))"
      ],
      "metadata": {
        "id": "LcwXhSl6Aj9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAEsB3uB62CX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import rcParams\n",
        "rcParams['figure.figsize'] = 8, 8\n",
        "font_size = 22\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import itertools\n",
        "class_names= [\"Promotional\",\"Neutral\",\"Discouraging\"]"
      ],
      "metadata": {
        "id": "gSmLO7ASSIjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "   \n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    \n",
        "    print(cm)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar(shrink=0.7)\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    #plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.xticks(tick_marks, classes)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\",\n",
        "                 fontsize=font_size)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('Predicted label',fontsize=font_size)\n",
        "    plt.xlabel('True label',fontsize=font_size)"
      ],
      "metadata": {
        "id": "_6WQCLlet8gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = list(test_data1[\"text\"])\n",
        "X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=max_seq_length)\n",
        "\n",
        "test_dataset = Dataset(X_test_tokenized)\n",
        "test_pred, _, _ = trainer.predict(test_dataset)\n",
        "y_pred = np.argmax(test_pred, axis=1)\n",
        "y_test = test_data1[\"label\"].to_numpy().astype(\"int\")\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plot_confusion_matrix(cnf_matrix, classes= class_names, normalize=True)\n",
        "plt.title('Test datasetA',fontsize=font_size)"
      ],
      "metadata": {
        "id": "QIrAiYhqTdi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = list(test_data2[\"text\"])\n",
        "X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=max_seq_length)\n",
        "\n",
        "test_dataset = Dataset(X_test_tokenized)\n",
        "test_pred, _, _ = trainer.predict(test_dataset)\n",
        "y_pred = np.argmax(test_pred, axis=1)\n",
        "y_test = test_data2[\"label\"].to_numpy().astype(\"int\")\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plot_confusion_matrix(cnf_matrix, classes= class_names, normalize=True)\n",
        "plt.title('Test datasetB',fontsize=font_size)\n"
      ],
      "metadata": {
        "id": "2aufyxFISjim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = list(test_data3[\"text\"])\n",
        "X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=max_seq_length)\n",
        "\n",
        "test_dataset = Dataset(X_test_tokenized)\n",
        "test_pred, _, _ = trainer.predict(test_dataset)\n",
        "y_pred = np.argmax(test_pred, axis=1)\n",
        "y_test = test_data3[\"label\"].to_numpy().astype(\"int\")\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plot_confusion_matrix(cnf_matrix, classes= class_names, normalize=True)\n",
        "plt.title('Test datasetA+B',fontsize=font_size)"
      ],
      "metadata": {
        "id": "1eQt6w54WADr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ":: stop here"
      ],
      "metadata": {
        "id": "SIes9ZIEWzNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn1wn49WYiZ2"
      },
      "source": [
        "# **Save the model**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_to_save = \"xlm-roberta-large-finetuned-dAB-002\""
      ],
      "metadata": {
        "id": "62XSdP4rxHFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr4BypZRZOQL"
      },
      "source": [
        "### Install git lfs\n",
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash \n",
        "!sudo apt-get install git-lfs\n",
        "!git lfs install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDyQQVmsGRFj"
      },
      "source": [
        "# save on huggingface"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM8Cqcafj94J"
      },
      "source": [
        "!git config --global user.email \"Susancheatham1@gmail.com\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sGmnB33TAqr"
      },
      "source": [
        "!huggingface-cli login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "sQBsYl65oONb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uf-xroNXvf7"
      },
      "source": [
        "dir_to_save = './' + model_name_to_save + '/'\n",
        "dir_to_save"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPC6HsqtapSa"
      },
      "source": [
        "# Save locally first\n",
        "model.save_pretrained(dir_to_save)\n",
        "tokenizer.save_pretrained(dir_to_save)\n",
        "trainer.save_model(dir_to_save)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agraW5QkbzLq"
      },
      "source": [
        "#!ls "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TDsw0uaxMxl"
      },
      "source": [
        "    # Files to expect...\n",
        "    # a config.json file, which saves the configuration of your model ;\n",
        "    # a pytorch_model.bin file, which is the PyTorch checkpoint (unless you can’t have it for some reason) ;\n",
        "    # a tf_model.h5 file, which is the TensorFlow checkpoint (unless you can’t have it for some reason) ;\n",
        "    # a special_tokens_map.json, which is part of your tokenizer save;\n",
        "    # a tokenizer_config.json, which is part of your tokenizer save;\n",
        "    # files named vocab.json, vocab.txt, merges.txt, or similar, which contain the vocabulary of your tokenizer, part of your tokenizer save;\n",
        "    # maybe a added_tokens.json, which is part of your tokenizer save.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxBkTroZcYKG"
      },
      "source": [
        "!huggingface-cli repo create model_name_to_save --yes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u8tZh-_uhB_"
      },
      "source": [
        "hface_dir_to_save = 'https://huggingface.co/Cheatham/'+ model_name_to_save + '/'\n",
        "print(hface_dir_to_save)\n",
        "\n",
        "model.push_to_hub(hface_dir_to_save)\n",
        "tokenizer.push_to_hub(hface_dir_to_save)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GX6Tk3YXyai0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON5vZhdJuhEl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxI0S8o2F0dZ"
      },
      "source": [
        "#####import os\n",
        "#### Mount Google Drive to this Notebook instance.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls drive/MyDrive/EU-JAV/Models"
      ],
      "metadata": {
        "id": "g__G28Q18cqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mB9UvjfIE6-"
      },
      "source": [
        "cd MyDrive/EU-JAV/Models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1NHiBE-i-pB"
      },
      "source": [
        "trainer.save_model(model_name_to_save)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p5iSBHEzL1h"
      },
      "source": [
        "#tokenizer.save_pretrained(\"EU-JAV-finetuned-xlmroberta-tokenizer\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynXYyejJzN00"
      },
      "source": [
        "#model.save_pretrained(\"EU-JAV-finetuned-xlmroberta-model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5XcSUMx4gvH"
      },
      "source": [
        "###drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZNiNBqF3P2u"
      },
      "source": [
        "#ls EU-JAV-models/EUJAV-tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0J-0Xqn3eZq"
      },
      "source": [
        "#ls EU-JAV-models/EUJAV-finetuned-roberta-model_uncased"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIjM_WRb-keM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}